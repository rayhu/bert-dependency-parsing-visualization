# BERT Attention Visualization

This project visualizes BERT's attention weights using network graphs. It helps understand how different layers and attention heads in BERT process and relate different tokens in a sentence.

## Environment Setup

1. Create a new conda environment:
   ```bash
    conda env create -f environment.yml
   ```

2. Activate the environment:
   ```bash
    conda activate bert_env
   ```
3. Run Jupyter Notebook:
   ```bash
    jupyter notebook
   ```
4. Run the cells in the notebook.
## Why Visualize BERT Attention Weights?

Visualizing BERT's attention weights through dependency parsing is crucial for understanding and interpreting how this powerful language model processes text. By examining attention patterns, researchers and practitioners can gain insights into how BERT captures linguistic relationships, identifies important contextual connections, and builds semantic representations. These visualizations serve as a valuable diagnostic tool for model interpretability, helping to demystify the "black box" nature of transformer models. They can reveal whether BERT is learning meaningful grammatical structures, focusing on relevant tokens, or potentially developing problematic biases. This understanding is essential for improving model performance, debugging issues, and ensuring responsible deployment of BERT-based applications in real-world scenarios.

