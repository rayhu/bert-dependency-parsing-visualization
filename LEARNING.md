# Papers

## Group Tokens
[Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference, Qin et al., 2024](https://arxiv.org/abs/2407.09722v1)

This paper has an innovative point of joint perplexity with multi-token considered together. We shall similarly group tokens.
It is key to understanding the semantic structural embedding.

## What did BERT Heads learn
[What Does BERT Look At? An Analysis of BERT's Attention, Clark et al., 2019](https://arxiv.org/abs/1906.04341)

It is about what different heads in different layers learned during training.

## Why BERT heads learn
[Identifying Semantic Induction Heads to Understand In-Context Learning, Ren et al., 2024](https://arxiv.org/pdf/2402.13055)

It is about how BERT heads was able to capture syntactical and semantical embeddings.

## Implicit Vocabulary which is critical for Semantic Representation
[Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs, Feucht et al., 2024](https://arxiv.org/abs/2406.20086)

It talks about finding implicit vocabulary from tokenization, and semantic representation. 

## Structural Probe for Finding Syntax
[A Structural Probe for Finding Syntax in Word Representations. Hewitt et al., 2019](https://aclanthology.org/N19-1419/)
The structure probe found that all syntax trees are implicitly represented in ELMo and BERT but not in traditional word embeddings!
The hierarchical structure of syntax trees is reflected geometrically in the transformed space.

## Recall with attention edge
[Dissecting Recall of Factual Associations in Auto-Regressive Language Models. Geva et al., 2023](https://arxiv.org/abs/2304.14767)

Attention heads encode subject-attribute mappings and play a crucial role in attribute extraction.
The last-subject position is enriched with multiple related attributes, facilitating flexible and contextualized retrieval.

## Jing Huang's Repo
[Demystifying Verbatim Memorization in Large Language Models., 2025](https://github.com/explanare/verbatim-memorization)

Study verbatim memorization. What about making a loss function to punish the verbatim memorization?

